{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of generative AI use\n",
    "please note that I have used generative AI to do formatting for markdown cells (mostly for latex in q6) and commenting on my code. \n",
    "I have not used generative ai for the creation of any of my code or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.275032\n",
      "         Iterations: 35\n",
      "coefficients:\n",
      "MISSING_FROM              -7.062601\n",
      "FROM_ADDR_WS             -27.003504\n",
      "TVD_SPACE_RATIO           19.877574\n",
      "LOTS_OF_MONEY             38.589734\n",
      "T_FILL_THIS_FORM_SHORT    41.764697\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### problem 8, task ai.\n",
    "\n",
    "spam_test = pd.read_csv('data/spam_test.csv')\n",
    "spam_train = pd.read_csv('data/spam_train.csv')\n",
    "response_train = spam_train['SPAM']\n",
    "response_test = spam_test['SPAM']\n",
    "spam_train = spam_train.drop('SPAM', axis=1)\n",
    "spam_test = spam_test.drop('SPAM', axis=1)\n",
    "spam_train = sm.add_constant(spam_train)\n",
    "spam_test = sm.add_constant(spam_test)\n",
    "\n",
    "model = sm.Logit(response_train, spam_train)\n",
    "\n",
    "result = model.fit()\n",
    "print(\"coefficients:\")\n",
    "print(result.params[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data accuracy:\n",
      "0.88\n",
      "training data perplexity:\n",
      "1.3165728804311108\n",
      "testing data accuracy:\n",
      "0.88\n",
      "testing data perplexity:\n",
      "2.1872171476770728\n"
     ]
    }
   ],
   "source": [
    "### problem 8, task aii.\n",
    "\n",
    "# Predict the probabilities for the training set.\n",
    "response_train_pred = result.predict(spam_train)\n",
    "# Binarize the predicted probabilities: 1 if >= 0.5, otherwise 0.\n",
    "response_train_pred_vals = np.array(response_train_pred)\n",
    "response_train_pred_vals[response_train_pred_vals >= 0.5] = 1\n",
    "response_train_pred_vals[response_train_pred_vals < 0.5] = 0\n",
    "\n",
    "# Small constant to prevent log(0).\n",
    "epsilon = 1e-15\n",
    "\n",
    "# Calculate perplexity from the log likelihood.\n",
    "loglikelihood = ((response_train_pred) * (response_train) + \n",
    "                 (1 - response_train_pred) * (1 - response_train) + epsilon)\n",
    "perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "\n",
    "print(\"training data accuracy:\")\n",
    "print(accuracy_score(response_train, response_train_pred_vals))\n",
    "print(\"training data perplexity:\")\n",
    "print(perplexity)\n",
    "\n",
    "# Repeat the process for the test data.\n",
    "response_pred = result.predict(spam_test)\n",
    "response_pred_vals = np.array(response_pred)\n",
    "response_pred_vals[response_pred >= 0.5] = 1\n",
    "response_pred_vals[response_pred < 0.5] = 0\n",
    "\n",
    "loglikelihood = ((response_pred) * (response_test) + \n",
    "                 (1 - response_pred) * (1 - response_test) + epsilon)\n",
    "perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "\n",
    "print(\"testing data accuracy:\")\n",
    "print(accuracy_score(response_test, response_pred_vals))\n",
    "print(\"testing data perplexity:\")\n",
    "print(perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 8, task aiii.\n",
    "\n",
    "$$\n",
    "P(Y=1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $P(Y=1 \\mid x)$ is the probability that the outcome $Y$ is 1 given the covariate vector $x$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients of the logistic regression model.\n",
    "- $x_1, x_2, \\dots, x_n$ are the elements of the covariate vector $x$.\n",
    "- The expression $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n$ represents the linear combination of the covariates weighted by their respective coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10.0\n",
      "Coefficients: [-3.68993091  0.         -2.07387487  0.          3.62870203  6.77857594]\n",
      "Baseline Accuracy: 0.8800\n",
      "Accuracy with lasso: 0.8800\n",
      "Perplexity: 1.40673495810779\n"
     ]
    }
   ],
   "source": [
    "### problem 8, task b.\n",
    "\n",
    "# Define a range of values for the regularization \n",
    "# parameter C in logistic regression.\n",
    "c_vals = np.logspace(-3, 3, 7)\n",
    "best_v_val = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Iterate over different values of C to find the \n",
    "# one that gives the best accuracy.\n",
    "for c in c_vals:\n",
    "    # Initialise and train the model on the training dataset.\n",
    "    model = LogisticRegression(penalty='l1', C=c, \n",
    "                               solver='liblinear')\n",
    "    model.fit(spam_train, response_train)\n",
    "    # Predict on the test set and calculate accuracy.\n",
    "    pred = model.predict(spam_test)\n",
    "    accuracy = accuracy_score(response_test, pred)\n",
    "    # Update the best C value necessary\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_c_val = c\n",
    "\n",
    "# Train a new model using the best C value found.\n",
    "best_model = LogisticRegression(penalty='l1', C=best_c_val, \n",
    "                                solver='liblinear')\n",
    "best_model.fit(spam_train, response_train)\n",
    "\n",
    "# Evaluate the best model's performance on the test set.\n",
    "y_pred = best_model.predict(spam_test)\n",
    "accuracy = \"{:.4f}\".format(accuracy_score(response_test, y_pred))\n",
    "# Calculate perplexity for the best model. \n",
    "perplexity = np.exp(-np.mean(np.log(\n",
    "    best_model.predict_proba(spam_test)\n",
    "    [np.arange(len(response_test)), response_test])))\n",
    "# Baseline accuracy for comparison (from last question)\n",
    "accuracy_base = \"{:.4f}\".format(accuracy_score(response_test, \n",
    "                                               response_pred_vals))\n",
    "\n",
    "print(\"Best C:\", best_c_val)\n",
    "print(\"Coefficients:\", best_model.coef_[0])\n",
    "print(\"Baseline Accuracy:\", accuracy_base)\n",
    "print(\"Accuracy with lasso:\", accuracy)\n",
    "print(\"Perplexity:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 8, task b (written).\n",
    "the predicted class probabilities from the unregularized regressor in Task a are much higher than they are from the regulated task probablilities in task b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "![problem 9](problem-9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Mean_Ad   StdDev_Ad  Mean_notAd  StdDev_notAd\n",
      "Feature                                                          \n",
      "bill_length_mm       38.124    2.781528      47.818      3.599472\n",
      "bill_depth_mm        18.336    1.204118      15.890      1.965441\n",
      "flipper_length_mm   188.880    6.320074     211.300     11.792855\n",
      "body_mass_g        3576.000  461.343148    4657.000    787.531017\n",
      "\n",
      "Adelie probability: 0.33766233766233766\n",
      "not Adelie probability: 0.6623376623376623\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing data from CSV files.\n",
    "train = pd.read_csv('data/penguins_train.csv')\n",
    "test = pd.read_csv('data/penguins_test.csv')\n",
    "\n",
    "# Recode species column to binary: 'Adelie' or 'notAdelie'.\n",
    "train_res = train['species'].apply(lambda x: 'Adelie' \n",
    "                                 if x == 'Adelie' else 'notAdelie')\n",
    "test_res  = test['species'].apply(lambda x: 'Adelie' \n",
    "                                 if x == 'Adelie' else 'notAdelie')\n",
    "\n",
    "# Separate the data into Adelie and not Adelie for the training set.\n",
    "adelie_data = train[train['species'] == 'Adelie']\n",
    "not_adelie_data = train[train['species'] != 'Adelie']\n",
    "\n",
    "# Remove the species column as it's now encoded in train_res and test_res.\n",
    "train = train.drop('species', axis=1)\n",
    "test = test.drop('species', axis=1)\n",
    "\n",
    "# Calculate descriptive statistics for each feature by species.\n",
    "stats = []\n",
    "for feature in ['bill_length_mm', 'bill_depth_mm', \n",
    "                'flipper_length_mm', 'body_mass_g']:\n",
    "    stats.append({\n",
    "        'Feature': feature,\n",
    "        'Mean_Ad': adelie_data[feature].mean(),\n",
    "        'StdDev_Ad': adelie_data[feature].std(),\n",
    "        'Mean_notAd': not_adelie_data[feature].mean(),\n",
    "        'StdDev_notAd': not_adelie_data[feature].std()\n",
    "    })\n",
    "\n",
    "# Convert stats to a DataFrame and set 'Feature' as the index.\n",
    "stats = pd.DataFrame(stats).set_index('Feature')\n",
    "print(stats)\n",
    "\n",
    "# Calculate the probability of being Adelie or not with Laplace smoothing.\n",
    "num_ad = len(adelie_data)\n",
    "num_not_ad = len(not_adelie_data)\n",
    "prob_ad = (num_ad + 1) / (num_ad + num_not_ad + 2)  \n",
    "prob_not_ad = (num_not_ad + 1) / (num_ad + num_not_ad + 2) \n",
    "\n",
    "# Print the calculated probabilities.\n",
    "print(\"\\nAdelie probability:\", prob_ad)\n",
    "print(\"not Adelie probability:\", prob_not_ad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 10, task b\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(x_i - \\mu_i)^2}{2\\sigma_i^2}\\right)\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $\\sigma_i^2$ is the standard deviation of the ith attribute in the adelle class\n",
    "- $\\mu_i$ is the mean of the ith attribute in the adelle class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Adelie\n",
      "1    Adelie\n",
      "2    Adelie\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### Problem 10, task c:\n",
    "\n",
    "def naive_bays_classifier(x, stats):\n",
    "    # Initialize probabilities with the prior probabilities of each class.\n",
    "    p_ad = prob_ad\n",
    "    p_not = prob_not_ad\n",
    "    # Calculate the likelihood of the data point for each feature given the\n",
    "    # class using Gaussian Naive Bayes formula for classes.\n",
    "    for feature in ['bill_length_mm', 'bill_depth_mm', \n",
    "                    'flipper_length_mm', 'body_mass_g']:\n",
    "        p_ad *= (1/(np.sqrt(2 * np.pi) * stats.loc[feature, 'StdDev_Ad']))*\\\n",
    "                 np.exp(-((x[feature] - stats.loc[feature, 'Mean_Ad'])**2) / \n",
    "                         (2 * stats.loc[feature, 'StdDev_Ad']**2))\n",
    "        p_not *= (1/(np.sqrt(2 * np.pi) * stats.loc[feature, 'StdDev_notAd']))*\\\n",
    "                  np.exp(-((x[feature] - stats.loc[feature, 'Mean_notAd'])**2) / \n",
    "                          (2 * stats.loc[feature, 'StdDev_notAd']**2))\n",
    "    # return the class with the higher probability.\n",
    "    return 'Adelie' if p_ad > p_not else 'notAdelie'\n",
    "\n",
    "# Apply the classifier and the first three predictions.\n",
    "pred = test.apply(lambda x: naive_bays_classifier(x, stats), axis=1)\n",
    "print(pred[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "### Problem 11 task a: \n",
    "\n",
    "- According to the authors, is discriminative learning better than generative learning?\n",
    "    - No, the autors argue that in contrast to the widely-held belief that \"in almost all situations, discriminative learning is better than generative learning\", as the training set size increases, one algorithm will imarge to be a better fit than the other. \n",
    "\n",
    "### Problem 11 task b: \n",
    "\n",
    "- Ng and Jordan denote by â„Žðºð‘’ð‘› and â„Žð·ð‘–ð‘  two models chosen by optimizing different objectives. Which two families do the authors discuss, and what are the (â„Žðºð‘’ð‘›, â„Žð·ð‘–ð‘ ) pairs for those models? What objectives are being optimised?\n",
    "    - the two families tha that the authors discuss are Gaussian/Normal and multinomial distributions. \n",
    "    - the (â„Žðºð‘’ð‘›, â„Žð·ð‘–ð‘ ) pairs For Gaussian/Normal distributions are Normal Discriminant Analysis (â„Žðºð‘’ð‘›) and logistic regression (â„Žð·ð‘–ð‘ ).\n",
    "    - the (â„Žðºð‘’ð‘›, â„Žð·ð‘–ð‘ ) pairs For multinomial distributions are Naive Bayes classifier (â„Žðºð‘’ð‘›) and logistic regression (â„Žð·ð‘–ð‘ ).\n",
    "    - hGen optimises the joint liklyhood of inputs and labels\n",
    "    - hDis either optimises the conditional likelihood p(y|x) or the 0-1 training error\n",
    "\n",
    "### Problem 11 task c: \n",
    "- Study Figure 1 in the paper. Explain what it suggests (see the last paragraph of the Introduction). Reflect on what this means for the families in Task b.\n",
    "    - figure 1 suggests that while initially, the generative model (naive Bayes) performs better due to its faster approach to its asymptotic error, as the number of training examples increases, the discriminative model (logistic regression) catches up and surpasses the generative model due to its lower asymptotic error.\n",
    "    - for the families in task b, this means that the size of hte dataset is an important metric for deciding which model to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "### problem 12 task a.\n",
    "\n",
    "No, the naieves bayes assumption does not hold as Naive Bayes assumes that x1 and x2 are independent. hoewever the probability of y is defined as a function that includes an interaction term between x1 and x2, thus x1 and x2 are not independant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy table:\n",
      "          NB      LR     LRi OptimalBays   Dummy\n",
      "n                                               \n",
      "8     0.5973  0.6837  0.6957      0.7572  0.5688\n",
      "16    0.6537  0.6334  0.6277      0.7572  0.4312\n",
      "32    0.7037  0.7337  0.7365      0.7572  0.5688\n",
      "64    0.6843  0.7531  0.7506      0.7572  0.5688\n",
      "128   0.7444  0.7533  0.7528      0.7572  0.5688\n",
      "256   0.7507    0.75  0.7511      0.7572  0.5688\n",
      "512   0.7525  0.7528  0.7532      0.7572  0.5688\n",
      "1024  0.7476  0.7505  0.7506      0.7572  0.5688\n",
      "2048  0.7518  0.7524  0.7527      0.7572  0.5688\n",
      "4096  0.7513   0.751  0.7509      0.7572  0.5688\n",
      "\n",
      "perplexity table:\n",
      "             NB        LR       LRi OptimalBays     Dummy\n",
      "n                                                        \n",
      "8     81.029145  1.804528   1.88989     1.63518       2.0\n",
      "16     2.557424  6.102342  2.221673     1.63518  2.139477\n",
      "32     1.954326  1.717563  1.694754     1.63518       2.0\n",
      "64     1.892499  1.653345  1.667628     1.63518  1.981256\n",
      "128    1.665193  1.657166  1.652697     1.63518  1.983639\n",
      "256    1.645215  1.645612   1.64588     1.63518  1.981105\n",
      "512    1.656468  1.650604  1.649511     1.63518  1.981119\n",
      "1024   1.653585  1.645227  1.645366     1.63518  1.984055\n",
      "2048   1.654582   1.64527  1.645372     1.63518  1.981121\n",
      "4096   1.648145  1.645596  1.645583     1.63518  1.981116\n"
     ]
    }
   ],
   "source": [
    "### problem 12 task b.\n",
    "\n",
    "# Define a range of dataset sizes as powers of 2 from 2^3 to 2^12.             f\n",
    "n_values = [2**i for i in range(3, 13)]\n",
    "\n",
    "# Initialize dataframes to hold accuracy and perplexity results \n",
    "# for different classifiers.\n",
    "out_table = pd.DataFrame(columns=['n', 'NB', 'LR', 'LRi',\n",
    "                                   'OptimalBays', 'Dummy'])\n",
    "out_table['n'] = n_values\n",
    "out_table_accuracy = out_table.set_index('n')\n",
    "out_table_perplexity = out_table.set_index('n')\n",
    "\n",
    "# Load the test set and separate features from the target variable.\n",
    "test = pd.read_csv('data/toy_test.csv')\n",
    "test_res = test['y']\n",
    "test = test.drop('y', axis=1)\n",
    "\n",
    "# Iterate over each dataset size to train models and evaluate performance.\n",
    "for n in n_values:\n",
    "    # Load the training set corresponding to the current size.\n",
    "    train = pd.read_csv('data/toy_train_' + str(n) + '.csv')\n",
    "    train_res = train['y']\n",
    "    train = train.drop('y', axis=1)\n",
    "\n",
    "    # Train and evaluate a Gaussian Naive Bayes classifier.\n",
    "    model = GaussianNB()\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    loglikelihood = model.predict_proba(test)[np.arange(len(test_res)), test_res]\n",
    "    perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "    \n",
    "    # Store the accuracy and perplexity in the respective dataframes.\n",
    "    out_table_accuracy.loc[n, 'NB'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'NB'] = perplexity\n",
    "    \n",
    "    # Train and evaluate a Logistic Regression classifier without regularization.\n",
    "    model = LogisticRegression(penalty='none')\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    loglikelihood = model.predict_proba(test)[np.arange(len(test_res)), test_res]\n",
    "    perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "    out_table_accuracy.loc[n, 'LR'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'LR'] = perplexity\n",
    "\n",
    "    # Train and evaluate a Logistic Regression classifier with L1 regularization.\n",
    "    model = LogisticRegression(penalty='l1', C=1, solver='liblinear')\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    loglikelihood = model.predict_proba(test)[np.arange(len(test_res)), test_res]\n",
    "    perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "    out_table_accuracy.loc[n, 'LRi'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'LRi'] = perplexity\n",
    "\n",
    "    # Define the Optimal Bayes classifier function using the given equation.\n",
    "    def optimal_bayes_classifier(x1, x2):\n",
    "        t = -0.5 - x1 + 1.5 * x2 + (x1 * x2) / 3\n",
    "        return 1 / (1 + np.exp(-t))\n",
    "    \n",
    "    # Apply the Optimal Bayes classifier to the test set.\n",
    "    probs = test.apply(lambda x: optimal_bayes_classifier(x['x1'], x['x2']),\n",
    "                        axis=1)\n",
    "    pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate and store perplexity and accuracy for the Optimal Bayes classifier.\n",
    "    loglikelihood = ((probs) * (test_res) + \n",
    "                     (1 - probs) * (1 - test_res) + epsilon)\n",
    "    perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "    out_table_perplexity.loc[n, 'OptimalBays'] = perplexity\n",
    "    out_table_accuracy.loc[n, 'OptimalBays'] = accuracy_score(test_res, pred)\n",
    "\n",
    "    # Train and evaluate a Dummy classifier that predicts the majority class.\n",
    "    model = DummyClassifier(strategy='prior')\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    probs = model.predict_proba(test)[:,1]\n",
    "\n",
    "    # Calculate and store perplexity and accuracy for the Dummy classifier.\n",
    "    loglikelihood = ((probs) * (test_res) + \n",
    "                     (1 - probs) * (1 - test_res) + epsilon)\n",
    "    perplexity = np.exp(-np.mean(np.log(loglikelihood)))\n",
    "    out_table_perplexity.loc[n, 'Dummy'] = perplexity\n",
    "    out_table_accuracy.loc[n, 'Dummy'] = accuracy_score(test_res, pred)\n",
    "\n",
    "# Output the accuracy and perplexity tables for comparison of classifiers.\n",
    "print(\"accuracy table:\")\n",
    "print(out_table_accuracy)\n",
    "print(\"\\nperplexity table:\")\n",
    "print(out_table_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Coefficient\n",
      "Feature             \n",
      "x1         -1.029608\n",
      "x2          1.448626\n",
      "x1 x2       0.345481\n"
     ]
    }
   ],
   "source": [
    "### Problem 12 task c.\n",
    "\n",
    "# Load the training and testing datasets.\n",
    "train = pd.read_csv('data/toy_train_4096.csv')\n",
    "test = pd.read_csv('data/toy_test.csv')\n",
    "\n",
    "# Prepare the features (x) and \n",
    "# target (y) for the training set.\n",
    "x_train = train.drop('y', axis=1)\n",
    "res_train = train['y']\n",
    "\n",
    "# Generate polynomial features with degree 2, \n",
    "# excluding bias (intercept) term.\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, \n",
    "                          include_bias=False)\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "\n",
    "# Train a Logistic Regression model without \n",
    "# regularization on polynomial features.\n",
    "model = LogisticRegression(penalty='none')\n",
    "model.fit(x_train_poly, res_train)\n",
    "\n",
    "# Retrieve and print the feature names and \n",
    "# corresponding coefficients from the model.\n",
    "features = poly.get_feature_names_out(x_train.columns)\n",
    "coefficients = pd.DataFrame({'Feature': features, \n",
    "                             'Coefficient': model.coef_[0]})\n",
    "coefficients = coefficients.set_index('Feature')\n",
    "print(coefficients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 12 Task C (Written Portion):\n",
    "\n",
    "The regression coefficients:\n",
    "\n",
    "- x1: -1.029608\n",
    "- x2: 1.448626\n",
    "- (x1 x2): 0.345481\n",
    "\n",
    "are very close, but not identical, to the actual model coefficients:\n",
    "\n",
    "- x1: -1\n",
    "- x2: 1.5\n",
    "- (x1 x2): 0.333333\n",
    "\n",
    "#### Model Analysis\n",
    "- **Which of the models above are probabilistic, discriminative, and generative?**\n",
    "    - The actual model and the Optimal Naive Bayes are probabilistic.\n",
    "    - The Naive Bayes model is generative.\n",
    "    - The logistic regression models are discriminative.\n",
    "    - The dummy model is a non-learning model, which could loosely be categorized as probabilistic.\n",
    "- **How do accuracy and perplexity (log-likelihood) compare?**\n",
    "    - We can see that as perplexity decreases, accuracy tends to increase.\n",
    "- **Is there a relation to the insights from the previous problem?**\n",
    "    - It is clear that with smaller datasets, the discriminative models tend to outperform the generative models. However, as the dataset size increases, the generative models begin to outperform the discriminative ones. This observation correlates with the discussion in Question 11.\n",
    "- **Why does logistic regression with the interaction term perform so well for larger datasets?**\n",
    "    - For larger datasets, logistic regression can estimate the actual model's coefficients with a high degree of accuracy by capturing the complex interactions more effectively.\n",
    "- **Does your dummy classifier ever outperform other classifiers, or do different classifiers outperform the optimal Bayes classifier?**\n",
    "    - No, the dummy classifier does not outperform other classifiers. Similarly, other classifiers do not outperform the optimal Bayes classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13\n",
    "\n",
    "![problem 13](problem-13.jpg)\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " .\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    " \n",
    " .\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    " \n",
    " .\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14\n",
    "\n",
    "### Problem 14 task a\n",
    "\n",
    "- the classification boundries for 1-NN are as follows \n",
    "    - 4 as the class changes from +1 to -1.\n",
    "    - 5.5 the class changes from -1 to +1.\n",
    "    - 10.5 the class changes from +1 to -1.\n",
    "    - 15.5 the class changes from -1 to +1.\n",
    "    - 17 the class changes from +1 to -1.\n",
    "\n",
    "- for 1NN error rate: \n",
    "    - there are no errors\n",
    "\n",
    "- the classification boundries for 3-NN are as follows \n",
    "    - 10.5 as the class changes from +1 to -1.\n",
    "\n",
    "- for 3NN error rate, \n",
    "    - Point 4 (5.0) will be misclassified as +1 because its neighbors (3.0, 6.0, 8.0) include three +1s.\n",
    "    - Similarly, point 11 (16.0) will be misclassified as -1 because its neighbors (13.0, 15.0, 18.0) include three -1s. \n",
    "    - giving an error rate of 2/14 = 1/7 \n",
    "\n",
    "### Problem 14 task b\n",
    "\n",
    "- the choise of k in kNN will have an effect on how much the model will generalise the data. \n",
    "\n",
    "- For small k: \n",
    "    - the model will classify all areas around training data points as whatever its closest training data point is, thi will result in all predictions beside outliers and anomalies getting classified as the same class as the outlier.\n",
    "    - for example, with k = 1, the model will classify any input as its closest neighbour. In the model trained on D, this would look like input 4.2 getting classfied as -1.\n",
    "\n",
    "- For Large k: \n",
    "    - with a large k, the model will smooth out the prediction over a larger area, this will ignore noise and lead to a more generalizable model. However it will miss and fine details of the data set.\n",
    "    - For example, with k equal to the size of the training dataset, the classifier will predict the majority class for all points, ignoring the input features entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 15\n",
    "### Problem 15 task a: \n",
    "\n",
    "by geometric intuition, we can see that the points from both classes can be separated with a vertical line between the x-coordinates of the class 1 and class -1 points (somewhere between 2 < x1 < 4).\n",
    "\n",
    "to get the separating hyperplane with the largest margin on the data set we will find the median of the x coordinates closest to this separating hyperplane D and (E,F) (as E,F are both class -1 with the same x1 coordinate)\n",
    "$$\n",
    "x1 = (2+4)/2\n",
    "$$\n",
    "$$\n",
    "x1 = 3\n",
    "$$\n",
    "\n",
    "thus the hyper plane has equation $x1 = 3.0$ and support vectors D,E,F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 16\n",
    "\n",
    "### Problem 16 task a\n",
    "Over these two weeks, I learned about discriminative and generative classifiers, how they work, and the differences between them. I also revisited k-nearest neighbor, decision trees, and SVM algorithms.\n",
    "\n",
    "Unfortunatly I didn't really have time to digest the full content covered. I have been at a conference for the first week and a half and have only really had time in the last three days prior to submission to do the homework. I was lucky that most of the topics were things I already had a basic understanding of, which allowed me to quickly progress through the problems. For the next week's homework, I will have more time for studying, and I hope to re-read these chapters and get a better understanding of the topics.\n",
    "\n",
    "### Problem 16 task b.\n",
    "about 15 hours\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
