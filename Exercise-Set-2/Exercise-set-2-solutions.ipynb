{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of genereative AI usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.63387898 -16.15985276 -14.36460009  22.88863495  26.0635997 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### problem 8, task ai.\n",
    "\n",
    "spam_test = pd.read_csv('data_E2/spam_test.csv')\n",
    "spam_train = pd.read_csv('data_E2/spam_train.csv')\n",
    "responce_train = spam_train['SPAM']\n",
    "responce_test = spam_test['SPAM']\n",
    "spam_train = spam_train.drop('SPAM', axis=1)\n",
    "spam_test = spam_test.drop('SPAM', axis=1)\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty='none')\n",
    "model.fit(spam_train, responce_train)\n",
    "coefficients = model.coef_\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8800\n",
      "Perplexity:  1.8897996260639411\n"
     ]
    }
   ],
   "source": [
    "### problem 8, task aii.\n",
    "\n",
    "pred = model.predict(spam_test)\n",
    "accuracy_base =\"{:.4f}\".format(accuracy_score(responce_test, pred))\n",
    "\n",
    "perplexity = np.exp(-np.mean(np.log(model.predict_proba(spam_test)[np.arange(len(responce_test)), responce_test])))\n",
    "\n",
    "print('Accuracy: ', accuracy_base) \n",
    "print('Perplexity: ', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 8, task aiii.\n",
    "\n",
    "$$\n",
    "P(Y=1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $P(Y=1 \\mid x)$ is the probability that the outcome $Y$ is 1 given the covariate vector $x$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients of the logistic regression model.\n",
    "- $x_1, x_2, \\dots, x_n$ are the elements of the covariate vector $x$.\n",
    "- The expression $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n$ represents the linear combination of the covariates weighted by their respective coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10.0\n",
      "Coefficients: [ 0.         -2.07440282  0.          3.63006359  6.77996696]\n",
      "Baseline Accuracy: 0.8800\n",
      "Accuracy with lasso: 0.8800\n",
      "Perplexity: 1.4067547873582138\n"
     ]
    }
   ],
   "source": [
    "### problem 8, task b.\n",
    "\n",
    "c_vals = np.logspace(-3, 3, 7)\n",
    "best_v_val = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for c in c_vals:\n",
    "    model = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n",
    "    model.fit(spam_train, responce_train)\n",
    "    pred = model.predict(spam_test)\n",
    "    accuracy = accuracy_score(responce_test, pred)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_c_val = c\n",
    "\n",
    "# Train with the best C\n",
    "best_model = LogisticRegression(penalty='l1', C=best_c_val, solver='liblinear')\n",
    "best_model.fit(spam_train, responce_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(spam_test)\n",
    "accuracy =\"{:.4f}\".format(accuracy_score(responce_test, y_pred))\n",
    "perplexity = np.exp(-np.mean(np.log(best_model.predict_proba(spam_test)[np.arange(len(responce_test)), responce_test])))\n",
    "\n",
    "print(\"Best C:\", best_c_val)\n",
    "print(\"Coefficients:\", best_model.coef_[0])\n",
    "print(\"Baseline Accuracy:\", accuracy_base)\n",
    "print(\"Accuracy with lasso:\", accuracy)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 8, task b (written).\n",
    "the predicted class probabilities from the unregularized regressor in Task a are much higher than they are from the regulated task probablilities in task b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Mean_Ad   StdDev_Ad  Mean_notAd  StdDev_notAd\n",
      "Feature                                                          \n",
      "bill_length_mm       38.124    2.781528      47.818      3.599472\n",
      "bill_depth_mm        18.336    1.204118      15.890      1.965441\n",
      "flipper_length_mm   188.880    6.320074     211.300     11.792855\n",
      "body_mass_g        3576.000  461.343148    4657.000    787.531017\n",
      "\n",
      "Adelie probability: 0.33766233766233766\n",
      "not Adelie probability: 0.6623376623376623\n"
     ]
    }
   ],
   "source": [
    "### problem 10, task a\n",
    "\n",
    "train = pd.read_csv('data_E2/penguins_train.csv')\n",
    "test = pd.read_csv('data_E2/penguins_test.csv')\n",
    "\n",
    "train_res = train['species'].apply(lambda x: 'Adelie' if x == 'Adelie' else 'notAdelie')\n",
    "test_res = test['species'].apply(lambda x: 'Adelie' if x == 'Adelie' else 'notAdelie')\n",
    "\n",
    "\n",
    "adelie_data = train[train['species'] == 'Adelie']\n",
    "not_adelie_data = train[train['species'] == 'notAdelie']\n",
    "\n",
    "train = train.drop('species', axis=1)\n",
    "test = test.drop('species', axis=1)\n",
    "\n",
    "stats = []\n",
    "\n",
    "for i in ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "    stats.append({\n",
    "        'Feature': i,\n",
    "        'Mean_Ad': adelie_data[i].mean(),\n",
    "        'StdDev_Ad': adelie_data[i].std(),\n",
    "        'Mean_notAd': not_adelie_data[i].mean(),\n",
    "        'StdDev_notAd': not_adelie_data[i].std()\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats)\n",
    "stats = stats.set_index('Feature')\n",
    "    \n",
    "print(stats)\n",
    "\n",
    "num_ad = len(adelie_data)\n",
    "num_not_ad = len(not_adelie_data)\n",
    "\n",
    "prob_ad = (num_ad + 1) / (num_ad+num_not_ad+2*1)\n",
    "prob_not_ad = (num_not_ad + 1) / (num_ad+num_not_ad+2*1)\n",
    "\n",
    "print()\n",
    "print(\"Adelie probability:\", prob_ad)\n",
    "print(\"not Adelie probability:\", prob_not_ad)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 10, task b\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{\\sqrt{2\\pi \\times \\text{StdDev}_{\\text{Ad}}^i}} \\exp\\left(-\\frac{(x_i - \\text{Mean}_{\\text{Ad}}^i)^2}{2 \\times \\text{StdDev}_{\\text{Ad}}^2}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Adelie\n",
      "1    Adelie\n",
      "2    Adelie\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### Problem 10, task c:\n",
    "\n",
    "def naive_bays_classifier(x, stats):\n",
    "    p_ad = prob_ad\n",
    "    p_not_ad = prob_not_ad\n",
    "    for i in ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "        p_ad *= (1 / (np.sqrt(2 * np.pi) * stats.loc[i, 'StdDev_Ad'])) * np.exp(-((x[i] - stats.loc[i, 'Mean_Ad'])**2) / (2 * stats.loc[i, 'StdDev_Ad']**2))\n",
    "        p_not_ad *= (1 / (np.sqrt(2 * np.pi) * stats.loc[i, 'StdDev_notAd'])) * np.exp(-((x[i] - stats.loc[i, 'Mean_notAd'])**2) / (2 * stats.loc[i, 'StdDev_notAd']**2))\n",
    "    return 'Adelie' if p_ad > p_not_ad else 'notAdelie'\n",
    "\n",
    "pred = test.apply(lambda x: naive_bays_classifier(x, stats), axis=1)\n",
    "print(pred[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "### Problem 11 task a: \n",
    "\n",
    "- According to the authors, is discriminative learning better than generative learning?\n",
    "    - No, the autors argue that in contrast to the widely-held belief that \"in almost all situations, discriminative learning is better than generative learning\", as the training set size increases, one algorithm will imarge to be a better fit than the other. \n",
    "\n",
    "### Problem 11 task b: \n",
    "\n",
    "- Ng and Jordan denote by ℎ𝐺𝑒𝑛 and ℎ𝐷𝑖𝑠 two models chosen by optimizing different objectives. Which two families do the authors discuss, and what are the (ℎ𝐺𝑒𝑛, ℎ𝐷𝑖𝑠) pairs for those models? What objectives are being optimised?\n",
    "    - the two families tha that the authors discuss are Gaussian/Normal and multinomial distributions. \n",
    "    - the (ℎ𝐺𝑒𝑛, ℎ𝐷𝑖𝑠) pairs For Gaussian/Normal distributions are Normal Discriminant Analysis (ℎ𝐺𝑒𝑛) and logistic regression (ℎ𝐷𝑖𝑠).\n",
    "    - the (ℎ𝐺𝑒𝑛, ℎ𝐷𝑖𝑠) pairs For multinomial distributions are Naive Bayes classifier (ℎ𝐺𝑒𝑛) and logistic regression (ℎ𝐷𝑖𝑠).\n",
    "    - hGen optimises the joint liklyhood of inputs and labels\n",
    "    - hDis either optimises the conditional likelihood p(y|x) or the 0-1 training error\n",
    "\n",
    "### Problem 11 task c: \n",
    "- Study Figure 1 in the paper. Explain what it suggests (see the last paragraph of the Introduction). Reflect on what this means for the families in Task b.\n",
    "    - figure 1 suggests that while initially, the generative model (naive Bayes) performs better due to its faster approach to its asymptotic error, as the number of training examples increases, the discriminative model (logistic regression) catches up and surpasses the generative model due to its lower asymptotic error.\n",
    "    - for the families in task b, this means that the size of hte dataset is an important metric for deciding which model to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "### problem 12 task a.\n",
    "\n",
    "No, the naieves bayes assumption does not hold as: \n",
    "1. x1, x2 are not discrete. \n",
    "2. Naive Bayes assumes that x1 and x2 are independent. hoewever the probability of y is defined as a function that includes an interaction term between x1 and x2, thus x1 and x2 are not independant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy table:\n",
      "          NB      LR     LRi OptimalBays   Dummy\n",
      "n                                               \n",
      "8     0.5973  0.6837  0.6957      0.5936  0.5688\n",
      "16    0.6537  0.6334  0.6277       0.656  0.4312\n",
      "32    0.7037  0.7337  0.7365      0.7046  0.5688\n",
      "64    0.6843  0.7531  0.7506      0.6845  0.5688\n",
      "128   0.7444  0.7533  0.7528      0.7447  0.5688\n",
      "256   0.7507    0.75  0.7511      0.7507  0.5688\n",
      "512   0.7525  0.7528  0.7532      0.7525  0.5688\n",
      "1024  0.7476  0.7505  0.7506      0.7476  0.5688\n",
      "2048  0.7518  0.7524  0.7527      0.7517  0.5688\n",
      "4096  0.7513   0.751  0.7509      0.7514  0.5688\n",
      "\n",
      "perplexity table:\n",
      "             NB        LR       LRi OptimalBays Dummy\n",
      "n                                                    \n",
      "8     81.029145  1.804528   1.88989     1.88989   inf\n",
      "16     2.557424  6.102342  2.221673    2.221673   inf\n",
      "32     1.954326  1.717563  1.694754    1.694754   inf\n",
      "64     1.892499  1.653345  1.667624    1.667624   inf\n",
      "128    1.665193  1.657166  1.652694    1.652694   inf\n",
      "256    1.645215  1.645612  1.645881    1.645881   inf\n",
      "512    1.656468  1.650604  1.649512    1.649512   inf\n",
      "1024   1.653585  1.645227  1.645365    1.645365   inf\n",
      "2048   1.654582   1.64527  1.645372    1.645372   inf\n",
      "4096   1.648145  1.645596  1.645584    1.645584   inf\n"
     ]
    }
   ],
   "source": [
    "### problem 12 task b.\n",
    "\n",
    "n_values = [2**i for i in range(3, 13)]\n",
    "out_table = pd.DataFrame(columns=['n', 'NB', 'LR', 'LRi', 'OptimalBays', 'Dummy'])\n",
    "\n",
    "out_table['n'] = n_values\n",
    "out_table_accuracy = out_table.set_index('n')\n",
    "out_table_perplexity = out_table.set_index('n')\n",
    "test = pd.read_csv('data_E2/toy_test.csv')\n",
    "test_res = test['y']\n",
    "test = test.drop('y', axis=1)\n",
    "\n",
    "def naive_bays_classifier(x, stats):\n",
    "    p_0 = prob_0\n",
    "    p_1 = prob_1\n",
    "    for i in ['x1', 'x2']:\n",
    "        p_0 *= (1 / (np.sqrt(2 * np.pi) * stats.loc[i, 'StdDev_0'])) * np.exp(-((x[i] - stats.loc[i, 'Mean_0'])**2) / (2 * stats.loc[i, 'StdDev_0']**2))\n",
    "        p_1 *= (1 / (np.sqrt(2 * np.pi) * stats.loc[i, 'StdDev_1'])) * np.exp(-((x[i] - stats.loc[i, 'Mean_1'])**2) / (2 * stats.loc[i, 'StdDev_1']**2))\n",
    "    return 0 if p_0 > p_1 else 1\n",
    "\n",
    "for n in n_values:\n",
    "    train = pd.read_csv('data_E2/toy_train_' + str(n) + '.csv')\n",
    "    train_res = train['y']\n",
    "    train = train.drop('y', axis=1)\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    perplexity = np.exp(-np.mean(np.log(model.predict_proba(test)[np.arange(len(test_res)), test_res])))\n",
    "    out_table_accuracy.loc[n, 'NB'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'NB'] = perplexity\n",
    "    \n",
    "    model = LogisticRegression(penalty='none')\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    perplexity = np.exp(-np.mean(np.log(model.predict_proba(test)[np.arange(len(test_res)), test_res])))\n",
    "    out_table_accuracy.loc[n, 'LR'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'LR'] = perplexity\n",
    "\n",
    "    model = LogisticRegression(penalty='l1', C=1, solver='liblinear')\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    perplexity = np.exp(-np.mean(np.log(model.predict_proba(test)[np.arange(len(test_res)), test_res])))\n",
    "    out_table_accuracy.loc[n, 'LRi'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'LRi'] = perplexity\n",
    "\n",
    "    stats = []\n",
    "    for i in ['x1', 'x2']:\n",
    "        stats.append({\n",
    "            'Feature': i,\n",
    "            'Mean_0': train[train_res == 0][i].mean(),\n",
    "            'StdDev_0': train[train_res == 0][i].std(),\n",
    "            'Mean_1': train[train_res == 1][i].mean(),\n",
    "            'StdDev_1': train[train_res == 1][i].std()\n",
    "        })\n",
    "    stats = pd.DataFrame(stats)\n",
    "    stats = stats.set_index('Feature')\n",
    "\n",
    "    num_0 = len(train[train_res == 0])\n",
    "    num_1 = len(train[train_res == 1])\n",
    "\n",
    "    prob_0 = (num_0 + 1) / (num_0+num_1+2*1)\n",
    "    prob_1 = (num_1 + 1) / (num_0+num_1+2*1)\n",
    "\n",
    "    pred = test.apply(lambda x: naive_bays_classifier(x, stats), axis=1)\n",
    "    perplexity = np.exp(-np.mean(np.log(model.predict_proba(test)[np.arange(len(test_res)), test_res])))\n",
    "\n",
    "    out_table_accuracy.loc[n, 'OptimalBays'] = accuracy_score(test_res, pred)\n",
    "    out_table_perplexity.loc[n, 'OptimalBays'] = perplexity\n",
    "\n",
    "\n",
    "\n",
    "    model = DummyClassifier(strategy='most_frequent')\n",
    "    model.fit(train, train_res)\n",
    "    pred = model.predict(test)\n",
    "    perplexity = np.exp(-np.mean(np.log(model.predict_proba(test)[np.arange(len(test_res)), test_res])))\n",
    "    out_table_perplexity.loc[n, 'Dummy'] = perplexity\n",
    "    out_table_accuracy.loc[n, 'Dummy'] = accuracy_score(test_res, pred)\n",
    "\n",
    "print(\"accuracy table:\")\n",
    "print(out_table_accuracy)\n",
    "print()\n",
    "print(\"perplexity table:\")\n",
    "print(out_table_perplexity)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
